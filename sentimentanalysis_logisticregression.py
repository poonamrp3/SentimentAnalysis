# -*- coding: utf-8 -*-
"""SentimentAnalysis_LogisticRegression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nsc1lzPRWZ23qjA4DNmV-AcbCmAKLBFI
"""

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score
import pandas as pd
import re
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Load dataset (Update the file path as needed)
file_path = '/content/drive/MyDrive/sentimentdata.csv.xls'  # Adjust path if necessary
df = pd.read_csv(file_path, encoding='ISO-8859-1')  # Use appropriate encoding

# ---- Stage 1: Data Cleaning ----
df['text'] = df['text'].astype(str)
df['text'].fillna('', inplace=True)

def preprocess_text(text):
    """
    Clean and preprocess the text data:
    - Convert to lowercase
    - Remove URLs
    - Remove non-word characters
    """
    text = text.lower()
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'\W', ' ', text)
    return text

df['cleaned_text'] = df['text'].apply(preprocess_text)

# ---- Stage 2: Handle Missing Values ----
df = df.dropna(subset=['sentiment'])  # Drop rows where 'sentiment' is NaN

# Encode sentiment labels into numerical values
sentiment_encoder = LabelEncoder()
df['sentiment'] = sentiment_encoder.fit_transform(df['sentiment'])

# Debugging: Check the Sentiment Encoder Classes
print("Sentiment Encoder Classes:", sentiment_encoder.classes_)

# ---- Convert Categorical Features ----
label_encoder = LabelEncoder()
df['Time of Tweet'] = label_encoder.fit_transform(df['Time of Tweet'])
df['Age of User'] = label_encoder.fit_transform(df['Age of User'])
df['Country'] = label_encoder.fit_transform(df['Country'])

# ---- Stage 3: Feature Extraction ----
vectorizer = TfidfVectorizer()
X_text = vectorizer.fit_transform(df['cleaned_text'])

# Convert TF-IDF matrix to DataFrame
X_text_df = pd.DataFrame(X_text.toarray(), columns=vectorizer.get_feature_names_out())

# ---- Stage 4: Combine Features ----
# Combine text features with other numerical features
other_features = df[['Time of Tweet', 'Age of User', 'Country']]
X = pd.concat([X_text_df, other_features.reset_index(drop=True)], axis=1)
y = df['sentiment']

# ---- Stage 5: Data Splitting ----
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# ---- Stage 6: Feature Scaling ----
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# ---- Stage 7: Model Training ----
model = LogisticRegression(max_iter=5000)
model.fit(X_train, y_train)

# ---- Stage 8: Model Evaluation ----
y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# ---- Debugging: Check y_pred and y_test ----
print("Sample of y_test:", y_test.head())
print("Sample of y_pred:", pd.Series(y_pred).head())

# Classification Report
print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=sentiment_encoder.classes_))

#Support Vector Machines(SVM)
from sklearn.svm import SVC
svm_model = SVC(kernel='linear')
svm_model.fit(X_train, y_train)
y_pred_svm = svm_model.predict(X_test)
print("Support vector Machines")
print(f"Accuracy: {accuracy_score(y_test, y_pred_svm):.2f}")
print(classification_report(y_test, y_pred_svm, target_names=sentiment_encoder.classes_))

# Random Forest
from sklearn.ensemble import RandomForestClassifier
random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)
random_forest_model.fit(X_train, y_train)
y_pred_rf = random_forest_model.predict(X_test)
print("Random Forest")
print(f"Accuracy: {accuracy_score(y_test, y_pred_rf):.2f}")
print(classification_report(y_test, y_pred_rf, target_names=sentiment_encoder.classes_))

"""1. **Random Forest**:
   - **Best Overall Accuracy**: 0.63, indicating it performed best in terms of overall accuracy.
   - **F1-Scores**: Shows a good balance, especially for the `positive` class, with a high precision of 0.78 and an F1-score of 0.69.
   - **Recall Issues**: Poor recall for the `negative` class (0.38), indicating that the model missed many actual negative instances. However, it performed well with `neutral` and `positive` classes.

2. **Logistic Regression**:
   - **Intermediate Accuracy**: 0.57.
   - **Balanced Performance**: Shows a reasonable balance across classes with a macro average F1-score of 0.57. It performs relatively well for `positive` sentiment with a precision of 0.63.
   - **Challenges**: Lower performance in recall for `negative` sentiment (0.49) and `neutral` sentiment (0.65).

3. **SVM**:
   - **Lowest Accuracy**: 0.56, slightly lower than Logistic Regression.
   - **Similar Performance**: Similar precision and recall scores to Logistic Regression but generally less effective in classification.
   - **Specific Performance**: Higher precision for `positive` sentiment (0.64) but lower recall for `negative` sentiment (0.48).


"""

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report
from sklearn.ensemble import RandomForestClassifier

# Define the parameter grid
param_grid = {
    'n_estimators': [50, 100, 150, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize the Random Forest model
rf = RandomForestClassifier(random_state=42)

# Set up Grid Search with cross-validation
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,
                           cv=5, n_jobs=-1, verbose=2, scoring='accuracy')

# Fit the Grid Search
grid_search.fit(X_train, y_train)

# Get the best parameters and model
best_params = grid_search.best_params_
best_rf_model = grid_search.best_estimator_

# Predict on the test set
y_pred_rf = best_rf_model.predict(X_test)

# Print results
print("Random Forest with Best Parameters")
print(f"Best Parameters: {best_params}")
print(f"Accuracy: {accuracy_score(y_test, y_pred_rf):.2f}")
print(classification_report(y_test, y_pred_rf, target_names=sentiment_encoder.classes_))